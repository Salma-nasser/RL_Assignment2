{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a591ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import sys, site\n",
    "sys.path.insert(0, site.getsitepackages()[0])\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e33aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbasmaa2003\u001b[0m (\u001b[33mbasmaa2003-cairo-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing-Wandb\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4c3fbd210d986d0a1d09d50e3e0af6ff012c590c\"\n",
    "# Start a new wandb run to track this script.\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"basmaa2003-cairo-university\",\n",
    "    project=\"my-awesome-project\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "    run.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = Path(\"videos\")\n",
    "video_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def should_record(episode_id: int) -> bool:\n",
    "    return episode_id == 0 or (episode_id + 1) % 50 == 0\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(video_dir),\n",
    "    episode_trigger=should_record,\n",
    "    name_prefix=\"cartpole-training\",\n",
    ")\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = QNetwork(n_observations, n_actions).to(device)\n",
    "target_net = QNetwork(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(20000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result: bool = False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    if non_final_mask.any():\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    else:\n",
    "        non_final_next_states = torch.empty((0, state_batch.size(-1)), dtype=state_batch.dtype, device=state_batch.device)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    if non_final_next_states.size(0) > 0:\n",
    "        with torch.no_grad():\n",
    "            next_state_policy_actions = policy_net(non_final_next_states).argmax(1, keepdim=True)\n",
    "            target_q_values = target_net(non_final_next_states).gather(1, next_state_policy_actions).squeeze(1)\n",
    "        next_state_values[non_final_mask] = target_q_values\n",
    "\n",
    "    expected_state_action_values = reward_batch + (next_state_values * GAMMA)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4520c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 400\n",
    "\n",
    "SOLVE_WINDOW = 100\n",
    "SOLVE_SCORE = 475\n",
    "best_average = 0.0\n",
    "solved = False\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset(seed=seed + i_episode)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward_tensor = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = None if done else torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward_tensor)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_length = t + 1\n",
    "            episode_durations.append(episode_length)\n",
    "            plot_durations()\n",
    "            recent_avg = np.mean(episode_durations[-SOLVE_WINDOW:]) if len(episode_durations) >= SOLVE_WINDOW else np.mean(episode_durations)\n",
    "            best_average = max(best_average, recent_avg)\n",
    "            if (i_episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {i_episode + 1}: length={episode_length}, avg={recent_avg:.1f}\")\n",
    "            if len(episode_durations) >= SOLVE_WINDOW and recent_avg >= SOLVE_SCORE:\n",
    "                print(f\"Solved CartPole after {i_episode + 1} episodes! Average length over last {SOLVE_WINDOW}: {recent_avg:.1f}\")\n",
    "                solved = True\n",
    "            break\n",
    "    if solved:\n",
    "        break\n",
    "\n",
    "if not solved:\n",
    "    print(f\"Stopped after {len(episode_durations)} episodes. Best average length: {best_average:.1f}\")\n",
    "else:\n",
    "    print(\"Training complete â€“ environment solved.\")\n",
    "\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585891cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def show_latest_video(video_directory: Path = video_dir) -> HTML:\n",
    "    video_files = sorted(video_directory.glob(\"*.mp4\"))\n",
    "    if not video_files:\n",
    "        raise FileNotFoundError(\"No recorded CartPole episodes yet.\")\n",
    "    latest_video = video_files[-1]\n",
    "    video_b64 = base64.b64encode(latest_video.read_bytes()).decode(\"ascii\")\n",
    "    return HTML(\n",
    "        f'<video width=\"640\" controls>'\n",
    "        f'<source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\" />'\n",
    "        \"Your browser does not support the video tag.\"\n",
    "        \"</video>\"\n",
    ")\n",
    "\n",
    "show_latest_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
